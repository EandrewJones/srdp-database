# This docker-compose file will simultaneously launch and link all the services required
# to run the CogSec Social Media Simulator. 

# You may change the port mappings on the local host, but altering the within-container
# ports may break container-to-container communication unless you also change the exposures.

# NOTE: None of the containers have a volume set up. This means there is no data persistence
# for any of the databases across container deploys/destroys. If persistence is desired, you
# must set up a localhost:container volume mapping.
version: "3.9"

services:

    # MySQL Database
    db:
        image: mysql/mysql-server:5.7
        restart: always
        env_file: 
            - .env
        ports:
            # <Ports exposed> : <MySQL Port running inside container>
            # Set to 3307 since 3306 occupied by native MySQL db running on
            # Evan's server. Can set back to 3306 on production cluster
            - '3307:3306'
        expose:
            # Opens port 3306 on the container
            - '3306'
        networks:
            - dbnet
        command: --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci
        volumes:
            - ./db/custom.cnf:/etc/my.cnf.d/90-my.cnf

    # MinIO S3 
    # s3:
    #     image: minio/minio
    #     restart: always
    #     env_file:
    #         - .env
    #     ports:
    #         - '9000:9000'
    #     expose:
    #         - "9000"
    #     command: server /data
    #     healthcheck:
    #         test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
    #         interval: 30s
    #         timeout: 20s
    #         retries: 3
    #     networks:
    #         dbnet:
    #             aliases:
    #                 - ${MINIO_HOSTNAME}
    # createbuckets:
    #     build: ./s3
    #     env_file:
    #         - .env
    #     depends_on:
    #         - s3
    #     links:
    #         - s3
    #     networks:
    #         - dbnet
    
    # Flask App
    web:
        build: 
            context: .
            args:
                - USERALE_AUTOSTART=${USERALE_AUTOSTART:-false}
        env_file:
            - .env
        depends_on: 
            - db
            - redis-server
            - logstash
            - es-searchbar
        links:
            # <service name> : <name within this container>
            - db
            - redis-server
            - es-searchbar
        ports:
            - '127.0.0.1:5000:5000'
        networks:
            - esnet
            - dbnet
        # volumes:
        #     - .:/cssms

    # Elasticsearch for Search Bar
    es-searchbar:
        build: ./search
        env_file:
            - .env
        ulimits:
            memlock:
                soft: -1
                hard: -1
        healthcheck:
            test: ["CMD", "curl","-s" ,"-f", "-u", "http://localhost:9200/_cluster/health"]
        ports:
            # Exposing port 9201 since 9200 is exposed for elasticsearch on esnet
            - '9201:9200'
            - '9301:9300'
        networks:
            - dbnet
        
    # Redis Stack for Tasks
    # Redis server
    redis-server:
        image: redis:3-alpine
        restart: always
        env_file:
            - .env
        ports:
            # <Ports exposed> : <Redis port running inside container>
            # Question: do we even need to map to local port since only flask app should
            # be communicating with this
            - '6379:6379'
        expose:
            - '6379'
        networks:
            - dbnet
        # volumes:
        #     TODO: set up a redis.conf file
        #     - ./redis.conf:/usr/local/etc/redis/redis.conf 
    # Workers  
    rq-worker1:
        build:
            context: "."
        depends_on:
            - redis-server
            - db
        links:
            - redis-server
            - db
        env_file:
            - .env
        restart: always
        entrypoint: venv/bin/rq
        # TODO: In a production environment you will probably want to have at
        # least as many workers as available CPUs
        command: worker --name worker1 -u redis://redis-server:6379/0 cssms-tasks
        networks:
            - dbnet
    rq-worker2:
        build:
            context: "."
        depends_on:
            - redis-server
            - db
        links:
            - redis-server
            - db
        env_file:
            - .env
        restart: always
        entrypoint: venv/bin/rq
        # TODO: In a production environment you will probably want to have at
        # least as many workers as available CPUs
        command: worker --name worker2 -u redis://redis-server:6379/0 cssms-tasks
        networks:
            - dbnet
    # Redis dashboard
    dashboard:
        build:
            context: "."
        ports:
            - 5555:5555
        depends_on:
            - redis-server
        links:
            - redis-server
        entrypoint: venv/bin/rq-dashboard
        command: --port 5555 --redis-url redis://redis-server:6379/0
        networks:
            - dbnet

    # ELK Stack for UserALE
    # Elasticsearch
    elasticsearch:
        build: ./elk/elasticsearch
        env_file:
            - .env
        ulimits:
            memlock:
                soft: -1
                hard: -1
        healthcheck:
            test: ["CMD", "curl","-s" ,"-f", "-u", "http://localhost:9200/_cluster/health"]
        # Mount volumes to backup elasticsearch data
        volumes:
            - esdata:/usr/share/elasticsearch/data
        ports:
            - '9200:9200'
            - '9300:9300'
        networks:
            - esnet
    # Logstash
    logstash:
        build: ./elk/logstash
        env_file: 
            - .env
        ports:
            - '8100:8100'
        volumes:
            - ./elk/logstash/pipeline/logstash-userale.conf:/usr/share/logstash/pipeline/logstash-userale.conf
            - ./elk/logstash/templates/userale.json:/usr/share/logstash/templates/userale.json
        depends_on:
            - "elasticsearch"
        networks:
            - esnet
    # Kibana
    kibana:
        build: ./elk/kibana
        env_file:
            - .env
        ports:
            - '5601:5601'
        depends_on:
            - "elasticsearch"
        networks:
            - esnet

# Set up networks
networks:
    esnet:
        external: 
            name: esnet
    dbnet:
        external:
            name: dbnet

# Names our volume
volumes: 
    my-db:
    esdata:
